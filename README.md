# local-rag
This is an attempt to build and PDF Question & Answering system using Llama-2 in local with help of langchain.

## Requirements
1. quantized llama-2 model on local:
2. langchain == 0.0.272
3. llama-cpp-python == 0.2.78
4. streamlit
